"""
modules/core/model_manager/services.py
Centralized model management service for loading, sharing, and managing AI models.

Supports:
- SentenceTransformer models (embeddings)
- Transformers models (T5, BERT, etc.)
- Custom model types
- Model sharing across modules
- GPU memory management
- Model caching and lifecycle management
"""

import logging
import time
import hashlib
import asyncio
import uuid
from typing import Dict, Any, Optional, List, Union
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
from core.error_utils import Result, error_message

# Module identity (must match manifest.json)
MODULE_ID = "core.model_manager"
logger = logging.getLogger(MODULE_ID)


class WorkerState(Enum):
    """Worker state enumeration."""
    IDLE = "idle"
    BUSY = "busy"
    LOADING = "loading"
    UNLOADING = "unloading"
    ERROR = "error"
    SHUTDOWN = "shutdown"


@dataclass
class WorkerTask:
    """Task for worker processing."""
    task_id: str
    task_type: str  # "embedding", "text_generation"
    model_id: str
    input_data: Any
    metadata: Dict[str, Any]
    created_at: float
    priority: int = 5


@dataclass
class WorkerResult:
    """Result from worker processing."""
    task_id: str
    worker_id: str
    success: bool
    data: Any = None
    error: str = None
    processing_time: float = 0.0
    metadata: Dict[str, Any] = None


class ModelWorker:
    """Individual worker that can load different model types dynamically."""
    
    def __init__(self, worker_id: str, device: str, model_manager_service):
        """Initialize model worker.
        
        Args:
            worker_id: Unique identifier for this worker
            device: Target device (e.g., 'cuda:0', 'cpu')
            model_manager_service: Reference to parent model manager
        """
        self.worker_id = worker_id
        self.device = device
        self.model_manager = model_manager_service
        self.logger = logging.getLogger(f"{MODULE_ID}.worker.{worker_id}")
        
        # State management
        self.state = WorkerState.IDLE
        self.current_model_id = None
        self.current_model = None
        self.is_preloaded = False  # Track if current model was preloaded
        self.last_activity = time.time()
        self.task_queue = asyncio.Queue()
        self.is_running = False
        self._worker_task = None
        
        # Performance tracking
        self.tasks_processed = 0
        self.total_processing_time = 0.0
        self.errors = 0
        self.model_switches = 0
        
        self.logger.info(f"Worker {worker_id} created on device {device}")
    
    async def start(self) -> bool:
        """Start the worker processing loop."""
        try:
            self.is_running = True
            self._worker_task = asyncio.create_task(self._worker_loop())
            self.logger.info(f"Worker {self.worker_id} started")
            return True
        except Exception as e:
            self.logger.error(f"Failed to start worker {self.worker_id}: {e}")
            return False
    
    async def stop(self):
        """Stop the worker and clean up resources."""
        self.logger.info(f"Stopping worker {self.worker_id}")
        self.is_running = False
        self.state = WorkerState.SHUTDOWN
        
        if self._worker_task and not self._worker_task.done():
            self._worker_task.cancel()
            try:
                await self._worker_task
            except asyncio.CancelledError:
                pass
        
        # Unload current model
        if self.current_model_id:
            await self._unload_model()
        
        self.logger.info(f"Worker {self.worker_id} stopped")
    
    async def submit_task(self, task: WorkerTask) -> bool:
        """Submit a task to this worker.
        
        Args:
            task: Task to process
            
        Returns:
            True if task was accepted
        """
        if not self.is_running or self.state == WorkerState.SHUTDOWN:
            return False
        
        try:
            await self.task_queue.put(task)
            self.logger.debug(f"Task {task.task_id} submitted to worker {self.worker_id}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to submit task to worker {self.worker_id}: {e}")
            return False
    
    async def _worker_loop(self):
        """Main worker processing loop - consumes from global job queue."""
        self.logger.info(f"Worker {self.worker_id} processing loop started")
        
        while self.is_running:
            try:
                # Try individual worker queue first (for direct worker assignment)
                try:
                    task = await asyncio.wait_for(
                        self.task_queue.get(), 
                        timeout=1.0  # Short timeout for individual queue
                    )
                    self.task_queue.task_done()
                    self.logger.debug(f"Worker {self.worker_id} got task from individual queue")
                except asyncio.TimeoutError:
                    # No individual tasks, try global job queue if available
                    if hasattr(self.model_manager, '_global_job_queue') and self.model_manager._global_job_queue is not None:
                        task = await asyncio.wait_for(
                            self.model_manager._global_job_queue.get(), 
                            timeout=self.model_manager.config.get("worker_pool.queue_timeout", 30)
                        )
                        self.model_manager._global_job_queue.task_done()
                        self.logger.debug(f"Worker {self.worker_id} got task from global queue")
                    else:
                        # No tasks available, continue loop
                        continue
                
                # Process the task
                result = await self._process_task(task)
                
                # Always use result queue for simplicity and reliability
                await self.model_manager._worker_result_queue.put(result)
                self.logger.debug(f"Routed result for task {result.task_id} via result queue")
                
            except asyncio.TimeoutError:
                # No tasks available, check for model timeout
                await self._check_model_timeout()
                continue
            except Exception as e:
                self.logger.error(f"Worker {self.worker_id} error in processing loop: {e}")
                self.errors += 1
                continue
        
        self.logger.info(f"Worker {self.worker_id} processing loop ended")
    
    async def _process_task(self, task: WorkerTask) -> WorkerResult:
        """Process a single task.
        
        Args:
            task: Task to process
            
        Returns:
            Processing result
        """
        start_time = time.time()
        self.state = WorkerState.BUSY
        self.last_activity = start_time
        
        try:
            # Ensure correct model is loaded
            if self.current_model_id != task.model_id:
                await self._switch_model(task.model_id)
            
            # Process based on task type
            if task.task_type == "embedding":
                result_data = await self._process_embedding_task(task)
            elif task.task_type == "text_generation":
                result_data = await self._process_text_generation_task(task)
            else:
                raise ValueError(f"Unknown task type: {task.task_type}")
            
            processing_time = time.time() - start_time
            self.tasks_processed += 1
            self.total_processing_time += processing_time
            self.state = WorkerState.IDLE
            
            self.logger.debug(f"Worker {self.worker_id} completed task {task.task_id} in {processing_time:.3f}s")
            
            return WorkerResult(
                task_id=task.task_id,
                worker_id=self.worker_id,
                success=True,
                data=result_data,
                processing_time=processing_time,
                metadata={"device": self.device, "model_id": task.model_id}
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.errors += 1
            self.state = WorkerState.ERROR
            
            self.logger.error(f"Worker {self.worker_id} failed to process task {task.task_id}: {e}")
            
            return WorkerResult(
                task_id=task.task_id,
                worker_id=self.worker_id,
                success=False,
                error=str(e),
                processing_time=processing_time
            )
    
    async def _switch_model(self, model_id: str):
        """Switch to a different model.
        
        Args:
            model_id: ID of model to load
        """
        self.state = WorkerState.LOADING
        
        # Unload current model if any
        if self.current_model_id and self.current_model_id != model_id:
            await self._unload_model()
        
        # Load new model
        await self._load_model(model_id)
        self.model_switches += 1
        
        self.logger.info(f"Worker {self.worker_id} switched to model {model_id}")
    
    async def switch_model(self, model_id: str):
        """Switch to a different model (public interface for preloading)."""
        await self._switch_model(model_id)
    
    async def _load_model(self, model_id: str):
        """Load a model on this worker with CUDA context management.
        
        Args:
            model_id: ID of model to load
        """
        try:
            # Clear CUDA cache and synchronize before loading
            if self.device.startswith("cuda"):
                try:
                    import torch
                    device_idx = int(self.device.split(':')[1]) if ':' in self.device else 0
                    torch.cuda.set_device(device_idx)
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize(device_idx)
                except (ImportError, RuntimeError) as e:
                    self.logger.warning(f"Worker {self.worker_id} CUDA setup warning: {e}")
            
            # Prevent CPU usage - refuse to load models on CPU
            if self.device == "cpu" and self.model_manager.config.get("worker_pool.require_gpu", True):
                raise RuntimeError(f"REFUSING to load {model_id} on CPU - GPU required. Fix GPU issues first.")
            
            # Load model instance directly on this worker's device
            # Each worker needs its own model instance, not shared models
            if model_id == "embedding":
                model_result = await self._load_worker_embedding_model(model_id)
            elif model_id == "t5_summarizer":
                model_result = await self._load_worker_text_generation_model(model_id)
            else:
                raise ValueError(f"Unknown model_id: {model_id}")
            
            if not model_result.success:
                raise RuntimeError(f"Failed to load model {model_id}: {model_result.error}")
            
            self.current_model = model_result.data
            self.current_model_id = model_id
            self.last_activity = time.time()
            
            self.logger.info(f"Worker {self.worker_id} loaded model {model_id}")
            
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id} failed to load model {model_id}: {e}")
            self.state = WorkerState.ERROR
            raise
    
    async def _unload_model(self):
        """Unload current model from this worker."""
        if self.current_model_id:
            self.state = WorkerState.UNLOADING
            
            # Actually move model to CPU to free GPU memory
            if self.current_model and self.device.startswith("cuda"):
                try:
                    if hasattr(self.current_model, 'to'):
                        self.current_model.to('cpu')
                        self.logger.info(f"Worker {self.worker_id} moved model {self.current_model_id} to CPU")
                    
                    import torch
                    torch.cuda.empty_cache()
                    self.logger.debug(f"Worker {self.worker_id} cleared CUDA cache")
                except Exception as e:
                    self.logger.warning(f"Worker {self.worker_id} failed to clear GPU memory: {e}")
            
            # Release model reference
            if self.current_model_id in self.model_manager._loaded_models:
                await self.model_manager.release_model(self.current_model_id)
            
            self.current_model = None
            self.current_model_id = None
            self.is_preloaded = False
            self.state = WorkerState.IDLE
            
            self.logger.info(f"Worker {self.worker_id} unloaded model completely")
    
    async def _load_worker_embedding_model(self, model_id: str):
        """Load embedding model instance specifically for this worker.
        
        Args:
            model_id: ID of the embedding model to load
            
        Returns:
            Result with model instance
        """
        try:
            # Get model configuration
            model_path = self.model_manager.config.get(f"models.{model_id}.local_path")
            if not model_path:
                return Result.error(
                    code="MODEL_PATH_NOT_CONFIGURED",
                    message=f"Model path not configured for {model_id}"
                )
            
            self.logger.info(f"Loading SentenceTransformer model from local path: {model_path} on {self.device}")
            
            # Load SentenceTransformer directly on this worker's device
            from sentence_transformers import SentenceTransformer
            
            model = SentenceTransformer(model_path, device=self.device)
            
            # Get model dimension for validation
            try:
                sample_embedding = model.encode(["test"], convert_to_tensor=True)
                dimension = sample_embedding.shape[1]
                self.logger.info(f"Successfully loaded embedding model: {model_id} (dimension: {dimension})")
            except Exception as e:
                self.logger.warning(f"Could not determine embedding dimension: {e}")
                dimension = None
            
            return Result.success(data={
                "model": model,
                "model_id": model_id,
                "dimension": dimension,
                "device": self.device,
                "worker_instance": True
            })
            
        except Exception as e:
            self.logger.error(f"Failed to load embedding model {model_id} on {self.device}: {e}")
            return Result.error(
                code="EMBEDDING_MODEL_LOAD_ERROR",
                message=f"Failed to load embedding model {model_id}",
                details={"error": str(e), "device": self.device}
            )
    
    async def _load_worker_text_generation_model(self, model_id: str):
        """Load text generation model instance specifically for this worker.
        
        Args:
            model_id: ID of the text generation model to load
            
        Returns:
            Result with model instance
        """
        try:
            # Get model configuration
            model_name = self.model_manager.config.get(f"models.{model_id}.name")
            if not model_name:
                return Result.error(
                    code="MODEL_NAME_NOT_CONFIGURED",
                    message=f"Model name not configured for {model_id}"
                )
            
            self.logger.info(f"Loading T5 model {model_name} on {self.device}")
            
            # Load T5 model directly on this worker's device
            from transformers import T5ForConditionalGeneration, AutoTokenizer
            
            model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            self.logger.info(f"Successfully loaded T5 model: {model_id} on {self.device}")
            
            return Result.success(data={
                "model": model,
                "tokenizer": tokenizer,
                "model_id": model_id,
                "device": self.device,
                "worker_instance": True
            })
            
        except Exception as e:
            self.logger.error(f"Failed to load T5 model {model_id} on {self.device}: {e}")
            return Result.error(
                code="T5_MODEL_LOAD_ERROR",
                message=f"Failed to load T5 model {model_id}",
                details={"error": str(e), "device": self.device}
            )
    
    async def _check_model_timeout(self):
        """Check if current model should be unloaded due to inactivity."""
        if not self.current_model_id:
            return
        
        # Don't unload preloaded models
        if self.is_preloaded:
            self.logger.debug(f"Worker {self.worker_id} keeping preloaded model {self.current_model_id}")
            return
        
        idle_timeout = self.model_manager.config.get("worker_pool.model_idle_timeout", 300)
        if (time.time() - self.last_activity) > idle_timeout:
            self.logger.info(f"Worker {self.worker_id} unloading idle model {self.current_model_id}")
            await self._unload_model()
    
    async def _process_embedding_task(self, task: WorkerTask) -> Dict[str, Any]:
        """Process an embedding task.
        
        Args:
            task: Embedding task
            
        Returns:
            Embedding results
        """
        if "model" not in self.current_model:
            raise RuntimeError("No embedding model loaded")
        
        model = self.current_model["model"]
        texts = task.input_data
        
        # Generate embeddings with CUDA synchronization
        try:
            # Ensure CUDA context is correct for this worker
            if self.device.startswith("cuda"):
                import torch
                device_idx = int(self.device.split(':')[1]) if ':' in self.device else 0
                torch.cuda.set_device(device_idx)
            
            if isinstance(texts, str):
                embeddings = model.encode([texts])
                result_embeddings = embeddings[0].tolist()
            else:
                embeddings = model.encode(texts)
                result_embeddings = [emb.tolist() for emb in embeddings]
            
            # Synchronize CUDA operations after processing
            if self.device.startswith("cuda"):
                torch.cuda.synchronize(device_idx)
                
        except Exception as e:
            self.logger.error(f"Worker {self.worker_id} embedding error: {e}")
            raise
        
        return {
            "embeddings": result_embeddings,
            "model_id": task.model_id,
            "dimension": len(result_embeddings[0] if isinstance(result_embeddings[0], list) else result_embeddings)
        }
    
    async def _process_text_generation_task(self, task: WorkerTask) -> Dict[str, Any]:
        """Process a text generation task.
        
        Args:
            task: Text generation task
            
        Returns:
            Generation results
        """
        if "model" not in self.current_model or "tokenizer" not in self.current_model:
            raise RuntimeError("No text generation model loaded")
        
        model = self.current_model["model"]
        tokenizer = self.current_model["tokenizer"]
        device = self.current_model["device"]
        
        input_text = task.input_data
        max_length = task.metadata.get("max_length", 128)
        
        # Tokenize input
        inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate
        try:
            import torch
        except ImportError:
            raise RuntimeError("PyTorch required for text generation")
            
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=max_length,
                num_beams=4,
                early_stopping=True,
                do_sample=False
            )
        
        # Decode result
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "model_id": task.model_id,
            "input_length": len(input_text),
            "output_length": len(generated_text)
        }
    
    def get_status(self) -> Dict[str, Any]:
        """Get worker status information.
        
        Returns:
            Worker status dictionary
        """
        avg_processing_time = (
            self.total_processing_time / max(1, self.tasks_processed)
        )
        
        return {
            "worker_id": self.worker_id,
            "device": self.device,
            "state": self.state.value,
            "current_model_id": self.current_model_id,
            "is_running": self.is_running,
            "tasks_processed": self.tasks_processed,
            "total_processing_time": self.total_processing_time,
            "average_processing_time": avg_processing_time,
            "errors": self.errors,
            "model_switches": self.model_switches,
            "last_activity": self.last_activity,
            "queue_size": self.task_queue.qsize()
        }


class ModelReference:
    """Tracks model usage and references."""
    def __init__(self, model_id: str, model_instance, model_config: Dict[str, Any]):
        self.model_id = model_id
        self.model_instance = model_instance
        self.model_config = model_config
        self.reference_count = 0
        self.last_accessed = time.time()
        self.created_at = time.time()
    
    def add_reference(self):
        """Add a reference to this model."""
        self.reference_count += 1
        self.last_accessed = time.time()
    
    def remove_reference(self):
        """Remove a reference from this model."""
        self.reference_count = max(0, self.reference_count - 1)
        self.last_accessed = time.time()
    
    def is_idle(self, idle_timeout: int) -> bool:
        """Check if model has been idle for too long."""
        return (time.time() - self.last_accessed) > idle_timeout

class ModelManagerService:
    """Centralized model management service."""
    
    def __init__(self, app_context):
        """Sync initialization - NO complex operations."""
        self.app_context = app_context
        self.logger = logger
        self.initialized = False
        self.config = {}
        
        # Model registry and cache
        self._loaded_models: Dict[str, ModelReference] = {}
        self._embedding_cache: Dict[str, Any] = {}
        self._cache_timestamps: Dict[str, float] = {}
        
        # Worker pool
        self._workers: Dict[str, ModelWorker] = {}
        self._worker_result_queue: Optional[asyncio.Queue] = None
        self._worker_pool_enabled = False
        self._next_worker_index = 0
        
        # Lazy loading references
        self._settings_service = None
        
        logger.info(f"{MODULE_ID} service created")
    
    @property
    def settings_service(self):
        """Lazy load settings service."""
        if self._settings_service is None:
            self._settings_service = self.app_context.get_service("core.settings.service")
            if not self._settings_service:
                logger.error(error_message(
                    module_id=MODULE_ID,
                    error_type="DEPENDENCY_UNAVAILABLE",
                    details="Settings service not available",
                    location="settings_service property"
                ))
        return self._settings_service
    
    def _flatten_pydantic_settings(self, pydantic_settings):
        """
        Convert nested Pydantic settings model to flattened dict format.
        
        This maintains backward compatibility with existing code that expects
        flattened keys like "models.embedding.device" instead of nested objects.
        
        Args:
            pydantic_settings: ModelManagerSettings instance
            
        Returns:
            dict: Flattened settings dictionary
        """
        config = {}
        
        # Extract embedding model settings
        embedding = pydantic_settings.embedding_model
        config.update({
            "models.embedding.type": embedding.type,
            "models.embedding.name": embedding.name,
            "models.embedding.local_path": embedding.local_path,
            "models.embedding.dimension": embedding.dimension,
            "models.embedding.device": embedding.device,
            "models.embedding.batch_size": embedding.batch_size,
            "models.embedding.shared": embedding.shared,
            "models.embedding.cache_embeddings": embedding.cache_embeddings,
        })
        
        # Extract T5 summarizer settings
        t5 = pydantic_settings.t5_summarizer
        config.update({
            "models.t5_summarizer.type": t5.type,
            "models.t5_summarizer.name": t5.name,
            "models.t5_summarizer.local_path": t5.local_path,
            "models.t5_summarizer.device": t5.device,
            "models.t5_summarizer.shared": t5.shared,
            "models.t5_summarizer.max_input_length": t5.max_input_length,
            "models.t5_summarizer.max_output_length": t5.max_output_length,
        })
        
        # Extract global device settings
        config.update({
            "device_preference": pydantic_settings.device_preference,
            "gpu_memory_fraction": pydantic_settings.gpu_memory_fraction,
            "allow_gpu_growth": pydantic_settings.allow_gpu_growth,
        })
        
        # Extract sharing settings
        sharing = pydantic_settings.sharing
        config.update({
            "sharing.enabled": sharing.enabled,
            "sharing.max_shared_models": sharing.max_shared_models,
            "sharing.unload_after_seconds": sharing.unload_after_seconds,
            "sharing.reference_counting": sharing.reference_counting,
        })
        
        # Extract embedding cache settings
        cache = pydantic_settings.embedding_cache
        config.update({
            "embedding_cache.enabled": cache.enabled,
            "embedding_cache.max_cache_size": cache.max_cache_size,
            "embedding_cache.ttl_seconds": cache.ttl_seconds,
            "embedding_cache.persist_to_disk": cache.persist_to_disk,
        })
        
        # Extract worker pool settings
        worker = pydantic_settings.worker_pool
        config.update({
            "worker_pool.enabled": worker.enabled,
            "worker_pool.num_workers": worker.num_workers,
            "worker_pool.devices": worker.devices,
            "worker_pool.batch_size": worker.batch_size,
            "worker_pool.queue_timeout": worker.queue_timeout,
            "worker_pool.worker_timeout": worker.worker_timeout,
            "worker_pool.preload_embeddings": worker.preload_embeddings,
            "worker_pool.auto_scaling": worker.auto_scaling,
            "worker_pool.load_balancing": worker.load_balancing,
            "worker_pool.require_gpu": worker.require_gpu,
            "worker_pool.model_priorities": worker.model_priorities,
            "worker_pool.device_affinity": worker.device_affinity,
            "worker_pool.memory_threshold": worker.memory_threshold,
            "worker_pool.model_idle_timeout": worker.model_idle_timeout,
        })
        
        # Extract general settings
        config.update({
            "enabled": pydantic_settings.enabled,
            "auto_initialize": pydantic_settings.auto_initialize,
            "log_model_usage": pydantic_settings.log_model_usage,
            "memory_monitoring": pydantic_settings.memory_monitoring,
        })
        
        return config
    
    async def initialize(self, settings=None) -> Result:
        """Initialize the model manager service with typed Pydantic settings."""
        if self.initialized:
            return Result.success(data={"status": "already_initialized"})
        
        try:
            # Handle typed Pydantic settings (preferred) or fallback to dict settings
            if settings:
                # Check if it's a Pydantic model instance (has model_dump method)
                if hasattr(settings, 'model_dump'):
                    # Convert Pydantic model to flattened dict format for backward compatibility
                    self.config = self._flatten_pydantic_settings(settings)
                    logger.info(f"{MODULE_ID}: Using typed Pydantic settings")
                else:
                    # Assume it's already a dict (legacy format)
                    self.config = settings
                    logger.info(f"{MODULE_ID}: Using dict settings (legacy)")
            else:
                # Fallback to default settings if no settings provided
                from .module_settings import DEFAULT_SETTINGS
                self.config = DEFAULT_SETTINGS
                logger.warning(f"{MODULE_ID}: No settings provided, using defaults")
            
            # Initialize device management
            await self._setup_device_management()
            
            # Initialize worker pool if enabled, otherwise preload models
            preload_result = None
            worker_pool_result = None
            
            if self.config.get("worker_pool.enabled", False):
                # Worker pool handles all model loading - skip preloading
                logger.info("Worker pool enabled - skipping preload, workers will handle model loading")
                worker_pool_result = await self._initialize_worker_pool()
            elif self.config.get("auto_initialize", True):
                # Traditional single-worker mode - preload models
                logger.info("Worker pool disabled - using traditional preloading")
                preload_result = await self._preload_configured_models()
            
            self.initialized = True
            logger.info(f"{MODULE_ID} service initialized successfully")
            
            init_data = {
                "status": "initialized",
                "models_configured": len(set(k.split(".")[1] for k in self.config.keys() if k.startswith("models.") and k.count(".") >= 2)),
                "sharing_enabled": self.config.get("sharing.enabled", False)
            }
            
            # Add preload information if available
            if preload_result and preload_result.success:
                init_data.update(preload_result.data)
            
            # Add worker pool information if available
            if worker_pool_result and worker_pool_result.success:
                init_data.update(worker_pool_result.data)
            
            return Result.success(data=init_data)
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="INITIALIZATION_ERROR",
                details=f"Error initializing {MODULE_ID} service: {str(e)}",
                location="initialize()"
            ))
            
            return Result.error(
                code="INITIALIZATION_ERROR",
                message=f"Failed to initialize {MODULE_ID} service",
                details={"error_type": type(e).__name__, "error": str(e)}
            )
    
    async def _setup_device_management(self) -> Result:
        """Setup device management for models."""
        try:
            device_preference = self.config.get("device_preference", "auto")
            gpu_memory_fraction = self.config.get("gpu_memory_fraction", 0.8)
            allow_gpu_growth = self.config.get("allow_gpu_growth", True)
            
            # Check for CUDA availability
            try:
                import torch
                cuda_available = torch.cuda.is_available()
                if cuda_available:
                    gpu_count = torch.cuda.device_count()
                    logger.info(f"CUDA available with {gpu_count} GPU(s)")
                    
                    # Configure GPU memory if needed
                    if device_preference in ["auto", "gpu"] and allow_gpu_growth:
                        logger.info(f"GPU memory fraction: {gpu_memory_fraction}, growth: {allow_gpu_growth}")
                else:
                    logger.info("CUDA not available, using CPU")
                    # Update device preference to reflect actual state for later checks
                    self.config["device_preference"] = "cpu"
                    if device_preference == "gpu":
                        logger.warning("GPU forced but CUDA unavailable, falling back to CPU")
            except ImportError:
                logger.info("PyTorch not available, using CPU for compatible models")
                # Update device preference to reflect actual state
                self.config["device_preference"] = "cpu"
            
            return Result.success(data={"device_setup": "complete"})
            
        except Exception as e:
            logger.warning(f"Device management setup failed: {e}")
            return Result.error(code="DEVICE_SETUP_ERROR", message=str(e))
    
    async def get_model(self, model_id: str) -> Result:
        """Get a shared model instance (supports both embedding and text generation models)."""
        try:
            # Check if model is already loaded
            if model_id in self._loaded_models:
                model_ref = self._loaded_models[model_id]
                model_ref.add_reference()
                logger.debug(f"Returning cached model: {model_id}")
                
                # Return structure depends on model type
                if isinstance(model_ref.model_instance, dict):
                    # Text generation model (has model, tokenizer, device structure)
                    return Result.success(data=model_ref.model_instance)
                else:
                    # Embedding model (single model instance)
                    return Result.success(data={
                        "model": model_ref.model_instance,
                        "model_id": model_id,
                        "cached": True,
                        "references": model_ref.reference_count
                    })
            
            # Check model type and delegate to appropriate loader
            model_type = self.config.get(f"models.{model_id}.type")
            if not model_type:
                return Result.error(
                    code="MODEL_NOT_CONFIGURED",
                    message=f"Model {model_id} not found in configuration"
                )
            
            if model_type == "embedding":
                return await self.get_embedding_model(model_id)
            elif model_type == "text2text":
                return await self.get_text_generation_model(model_id)
            else:
                return Result.error(
                    code="UNSUPPORTED_MODEL_TYPE",
                    message=f"Model type '{model_type}' not supported"
                )
                
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="GET_MODEL_ERROR",
                details=f"Error getting model {model_id}: {str(e)}",
                location="get_model()"
            ))
            
            return Result.error(
                code="GET_MODEL_ERROR",
                message="Failed to get model",
                details={"model_id": model_id, "error": str(e)}
            )
    
    async def get_model_config(self, model_id: str) -> Result:
        """Get model configuration."""
        try:
            model_type = self.config.get(f"models.{model_id}.type")
            if not model_type:
                return Result.error(
                    code="MODEL_NOT_CONFIGURED",
                    message=f"Model {model_id} not found in configuration"
                )
            
            # Extract model configuration
            model_config = {}
            for key, value in self.config.items():
                if key.startswith(f"models.{model_id}."):
                    config_key = key.replace(f"models.{model_id}.", "")
                    model_config[config_key] = value
            
            return Result.success(data=model_config)
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="GET_MODEL_CONFIG_ERROR",
                details=f"Error getting model config {model_id}: {str(e)}",
                location="get_model_config()"
            ))
            
            return Result.error(
                code="GET_MODEL_CONFIG_ERROR",
                message="Failed to get model configuration",
                details={"model_id": model_id, "error": str(e)}
            )
    
    async def get_text_generation_model(self, model_id: str = "t5_summarizer") -> Result:
        """Get a shared text generation model instance."""
        try:
            # Check if model is already loaded
            if model_id in self._loaded_models:
                model_ref = self._loaded_models[model_id]
                model_ref.add_reference()
                logger.debug(f"Returning cached text generation model: {model_id}")
                return Result.success(data={
                    "model": model_ref.model_instance["model"],
                    "tokenizer": model_ref.model_instance["tokenizer"],
                    "device": model_ref.model_instance["device"],
                    "config": model_ref.model_config,
                    "model_id": model_id,
                    "cached": True,
                    "references": model_ref.reference_count
                })
            
            # Check if model is configured using flat structure
            model_type = self.config.get(f"models.{model_id}.type")
            if not model_type:
                return Result.error(
                    code="MODEL_NOT_CONFIGURED",
                    message=f"Model {model_id} not found in configuration"
                )
            
            if model_type != "text2text":
                return Result.error(
                    code="INVALID_MODEL_TYPE",
                    message=f"Model {model_id} is not a text generation model"
                )
            
            # Load T5 model
            model_result = await self._load_t5_model(model_id)
            if not model_result.success:
                return model_result
            
            return model_result
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="GET_TEXT_GENERATION_MODEL_ERROR",
                details=f"Error getting text generation model {model_id}: {str(e)}",
                location="get_text_generation_model()"
            ))
            
            return Result.error(
                code="GET_TEXT_GENERATION_MODEL_ERROR",
                message="Failed to get text generation model",
                details={"model_id": model_id, "error": str(e)}
            )

    async def get_embedding_model(self, model_id: str = "embedding") -> Result:
        """Get a shared embedding model instance."""
        try:
            # Prevent CPU usage - check device preference
            device_preference = self.config.get("device_preference", "auto")
            if (device_preference == "cpu" and 
                self.config.get("worker_pool.require_gpu", True)):
                return Result.error(
                    code="CPU_USAGE_DISABLED",
                    message="REFUSING to load embedding model on CPU - GPU required. Fix GPU issues first.",
                    details={"device_preference": device_preference, "require_gpu": True}
                )
            # Check if model is already loaded
            if model_id in self._loaded_models:
                model_ref = self._loaded_models[model_id]
                model_ref.add_reference()
                logger.debug(f"Returning cached embedding model: {model_id}")
                return Result.success(data={
                    "model": model_ref.model_instance,
                    "model_id": model_id,
                    "cached": True,
                    "references": model_ref.reference_count
                })
            
            # Check if model is configured using flat structure
            model_type = self.config.get(f"models.{model_id}.type")
            if not model_type:
                return Result.error(
                    code="MODEL_NOT_CONFIGURED",
                    message=f"Model {model_id} not found in configuration"
                )
            
            if model_type != "embedding":
                return Result.error(
                    code="INVALID_MODEL_TYPE",
                    message=f"Model {model_id} is not an embedding model"
                )
            
            # Load SentenceTransformer model
            model_result = await self._load_sentence_transformer(model_id)
            if not model_result.success:
                return model_result
            
            return model_result
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="GET_EMBEDDING_MODEL_ERROR",
                details=f"Error getting embedding model {model_id}: {str(e)}",
                location="get_embedding_model()"
            ))
            
            return Result.error(
                code="GET_EMBEDDING_MODEL_ERROR",
                message="Failed to get embedding model",
                details={"model_id": model_id, "error": str(e)}
            )
    
    async def _load_t5_model(self, model_id: str) -> Result:
        """Load a T5 text generation model."""
        try:
            logger.info(f"Loading T5 model: {model_id}")
            
            # Get model configuration
            model_name = self.config.get(f"models.{model_id}.name")
            local_path = self.config.get(f"models.{model_id}.local_path")
            device_setting = self.config.get(f"models.{model_id}.device", "auto")
            max_input_length = self.config.get(f"models.{model_id}.max_input_length", 512)
            max_output_length = self.config.get(f"models.{model_id}.max_output_length", 128)
            
            if not model_name and not local_path:
                return Result.error(
                    code="INVALID_MODEL_CONFIG",
                    message=f"T5 model {model_id} requires either name or local_path"
                )
            
            # Import transformers here to avoid hard dependency
            try:
                from transformers import AutoTokenizer, T5ForConditionalGeneration
                import torch
                import os
            except ImportError as e:
                return Result.error(
                    code="MISSING_DEPENDENCY",
                    message=f"transformers library required for T5 models: {str(e)}"
                )
            
            # Load from local path if available, otherwise from HuggingFace
            try:
                if local_path and os.path.exists(local_path):
                    logger.info(f"Loading T5 model from local path: {local_path}")
                    tokenizer = AutoTokenizer.from_pretrained(local_path, local_files_only=True)
                    model = T5ForConditionalGeneration.from_pretrained(local_path, local_files_only=True)
                elif model_name:
                    logger.info(f"Loading T5 model from HuggingFace: {model_name}")
                    tokenizer = AutoTokenizer.from_pretrained(model_name)
                    model = T5ForConditionalGeneration.from_pretrained(model_name)
                else:
                    return Result.error(
                        code="MODEL_NOT_FOUND",
                        message=f"T5 model {model_id} not found: local_path doesn't exist and no model_name provided"
                    )
            except ImportError as ie:
                if "sentencepiece" in str(ie).lower():
                    return Result.error(
                        code="MISSING_SENTENCEPIECE",
                        message="T5 models require the sentencepiece library. Install with: pip install sentencepiece",
                        details={"error": str(ie), "solution": "Run: pip install sentencepiece && python -c 'import sentencepiece; print(\"SentencePiece installed successfully\")'"}
                    )
                else:
                    return Result.error(
                        code="IMPORT_ERROR", 
                        message=f"Failed to import required dependencies: {str(ie)}",
                        details={"error": str(ie)}
                    )
            
            # Determine device
            device = await self._determine_device(model_id)
            if device_setting == "cpu":
                device = "cpu"
                logger.info(f"Forcing CPU mode for T5 model {model_id}")
            
            # Move model to device
            device_torch = torch.device(device)
            model = model.to(device_torch)
            
            # Create model instance structure
            model_instance = {
                "model": model,
                "tokenizer": tokenizer,
                "device": device_torch
            }
            
            # Create model configuration
            model_config = {
                "model_id": model_id,
                "model_name": model_name or local_path,
                "max_input_length": max_input_length,
                "max_output_length": max_output_length,
                "device": device,
                "model_type": "text2text"
            }
            
            # Create model reference
            model_ref = ModelReference(model_id, model_instance, model_config)
            model_ref.add_reference()
            self._loaded_models[model_id] = model_ref
            
            logger.info(f"T5 model {model_id} loaded successfully on {device}")
            
            return Result.success(data={
                "model": model,
                "tokenizer": tokenizer,
                "device": device_torch,
                "config": model_config,
                "model_id": model_id,
                "cached": False,
                "references": model_ref.reference_count
            })
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="T5_MODEL_LOAD_ERROR",
                details=f"Failed to load T5 model {model_id}: {str(e)}",
                location="_load_t5_model()"
            ))
            
            return Result.error(
                code="T5_MODEL_LOAD_ERROR",
                message=f"Failed to load T5 model {model_id}",
                details={"model_id": model_id, "error": str(e)}
            )

    async def _load_sentence_transformer(self, model_id: str) -> Result:
        """Load a SentenceTransformer model."""
        try:
            from sentence_transformers import SentenceTransformer
            import os
            
            # Get model configuration from flat settings
            model_name = self.config.get(f"models.{model_id}.name")
            local_path = self.config.get(f"models.{model_id}.local_path")
            device = await self._determine_device(model_id)
            
            # Load from local path if available, otherwise from HuggingFace
            if local_path and os.path.exists(local_path):
                logger.info(f"Loading SentenceTransformer model from local path: {local_path} on {device}")
                model = SentenceTransformer(local_path, device=device)
            elif model_name:
                logger.info(f"Loading SentenceTransformer model from HuggingFace: {model_name} on {device}")
                model = SentenceTransformer(model_name, device=device)
            else:
                return Result.error(
                    code="MODEL_NOT_FOUND",
                    message=f"SentenceTransformer model {model_id} not found: no local_path and no model_name provided"
                )
            
            # Test the model
            test_embedding = model.encode("test")
            expected_dim = self.config.get(f"models.{model_id}.dimension", len(test_embedding))
            
            if len(test_embedding) != expected_dim:
                logger.warning(f"Model dimension mismatch: expected {expected_dim}, got {len(test_embedding)}")
            
            # Register the model - store flat config reference
            model_config = {"id": model_id, "name": model_name, "dimension": len(test_embedding)}
            model_ref = ModelReference(model_id, model, model_config)
            model_ref.add_reference()
            self._loaded_models[model_id] = model_ref
            
            logger.info(f"Successfully loaded embedding model: {model_id} (dimension: {len(test_embedding)})")
            
            return Result.success(data={
                "model": model,
                "model_id": model_id,
                "cached": False,
                "references": model_ref.reference_count,
                "dimension": len(test_embedding),
                "device": str(device)
            })
            
        except ImportError:
            return Result.error(
                code="MISSING_DEPENDENCY",
                message="sentence-transformers library not installed",
                details={"install_command": "pip install sentence-transformers"}
            )
        except Exception as e:
            return Result.error(
                code="MODEL_LOAD_ERROR",
                message=f"Failed to load SentenceTransformer model: {model_name}",
                details={"error": str(e)}
            )
    
    async def _determine_device(self, model_id: str) -> str:
        """Determine the best device for a model."""
        device_preference = self.config.get("device_preference", "auto")
        model_device = self.config.get(f"models.{model_id}.device", "auto")
        
        # Check for force CPU
        if device_preference == "cpu" or model_device == "cpu":
            return "cpu"
        
        # Check model-specific device setting
        if model_device != "auto":
            return model_device
        
        # Check for force GPU
        if device_preference == "gpu":
            try:
                import torch
                if torch.cuda.is_available():
                    return "cuda:0"
                else:
                    logger.warning("GPU forced but CUDA unavailable, falling back to CPU")
                    return "cpu"
            except ImportError:
                logger.warning("GPU forced but PyTorch unavailable, falling back to CPU")
                return "cpu"
        
        # Auto-detect best device
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda:0"
        except ImportError:
            pass
        
        return "cpu"
    
    async def _preload_configured_models(self) -> Result:
        """Preload models that are configured for auto-initialization."""
        try:
            preloaded_models = []
            
            # Check for embedding models to preload
            embedding_model_id = "embedding"
            if self.config.get(f"models.{embedding_model_id}.type") == "embedding":
                logger.info(f"Auto-preloading embedding model: {embedding_model_id}")
                model_result = await self.get_embedding_model(embedding_model_id)
                if model_result.success:
                    preloaded_models.append(embedding_model_id)
                    logger.info(f"Successfully preloaded embedding model: {embedding_model_id}")
                else:
                    logger.warning(f"Failed to preload embedding model {embedding_model_id}: {model_result.error}")
            
            # Check for T5 models to preload
            t5_model_id = "t5_summarizer"
            if self.config.get(f"models.{t5_model_id}.type") == "text2text":
                # Only preload T5 if specifically requested (it's large)
                if self.config.get(f"models.{t5_model_id}.preload", False):
                    logger.info(f"Auto-preloading T5 model: {t5_model_id}")
                    model_result = await self.get_text_generation_model(t5_model_id)
                    if model_result.success:
                        preloaded_models.append(t5_model_id)
                        logger.info(f"Successfully preloaded T5 model: {t5_model_id}")
                    else:
                        logger.warning(f"Failed to preload T5 model {t5_model_id}: {model_result.error}")
            
            return Result.success(data={
                "preloaded_models": preloaded_models,
                "total_preloaded": len(preloaded_models)
            })
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="PRELOAD_ERROR",
                details=f"Error preloading models: {str(e)}",
                location="_preload_configured_models()"
            ))
            return Result.error(
                code="PRELOAD_ERROR",
                message="Failed to preload configured models",
                details={"error": str(e)}
            )
    
    async def _initialize_worker_pool(self) -> Result:
        """Initialize the worker pool for parallel processing."""
        try:
            num_workers = self.config.get("worker_pool.num_workers", 2)
            devices = self.config.get("worker_pool.devices", ["cuda:0", "cuda:1"])
            
            # Validate device availability
            available_devices = []
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_count = torch.cuda.device_count()
                    for device in devices:
                        if device == "cpu":
                            available_devices.append(device)
                        elif device.startswith("cuda:"):
                            gpu_idx = int(device.split(":")[1])
                            if gpu_idx < gpu_count:
                                available_devices.append(device)
                            else:
                                logger.warning(f"GPU device {device} not available (only {gpu_count} GPUs)")
                        else:
                            logger.warning(f"Unknown device type: {device}")
                else:
                    if self.config.get("worker_pool.require_gpu", True):
                        logger.error("CUDA not available and GPU required - CPU usage disabled")
                        return Result.error(
                            code="GPU_REQUIRED_NOT_AVAILABLE",
                            message="GPU required but CUDA not available - check GPU status",
                            details={"require_gpu": True, "cuda_available": False}
                        )
                    else:
                        logger.warning("CUDA not available, using CPU fallback")
                        available_devices = ["cpu"] * num_workers
            except ImportError:
                if self.config.get("worker_pool.require_gpu", True):
                    logger.error("PyTorch not available and GPU required - FAILING")
                    return Result.error(
                        code="GPU_REQUIRED_PYTORCH_MISSING",
                        message="GPU required but PyTorch not available",
                        details={"require_gpu": True, "pytorch_available": False}
                    )
                else:
                    logger.warning("PyTorch not available, using CPU fallback")
                    available_devices = ["cpu"] * num_workers
            
            if not available_devices:
                available_devices = ["cpu"]
            
            # Limit workers to available devices
            actual_workers = min(num_workers, len(available_devices))
            
            # Create result queue and global job queue
            self._worker_result_queue = asyncio.Queue()
            self._global_job_queue = asyncio.Queue()
            
            # Create workers
            created_workers = 0
            for i in range(actual_workers):
                device = available_devices[i % len(available_devices)]
                worker_id = f"worker_{i}"
                
                try:
                    worker = ModelWorker(worker_id, device, self)
                    if await worker.start():
                        self._workers[worker_id] = worker
                        created_workers += 1
                        logger.info(f"Created worker {worker_id} on device {device}")
                    else:
                        logger.error(f"Failed to start worker {worker_id} on device {device}")
                except Exception as e:
                    logger.error(f"Error creating worker {worker_id}: {e}")
            
            if created_workers == 0:
                logger.warning("No workers created, worker pool disabled")
                self._worker_pool_enabled = False
                return Result.error(
                    code="WORKER_POOL_INIT_FAILED",
                    message="Failed to create any workers"
                )
            
            self._worker_pool_enabled = True
            logger.info(f"Worker pool initialized with {created_workers} workers")
            
            # Preload embedding models if configured
            preloaded_workers = 0
            if self.config.get("worker_pool.preload_embeddings", False):
                logger.info("Preloading embedding models on all workers...")
                for worker_id, worker in self._workers.items():
                    try:
                        await worker.switch_model("embedding")
                        # Mark as preloaded to prevent auto-unloading
                        worker.is_preloaded = True
                        preloaded_workers += 1
                        logger.info(f"Preloaded embedding model on worker {worker_id}")
                    except Exception as e:
                        logger.warning(f"Failed to preload embedding on worker {worker_id}: {e}")
                
                logger.info(f"Preloaded embedding models on {preloaded_workers}/{created_workers} workers")
            
            return Result.success(data={
                "worker_pool_enabled": True,
                "workers_created": created_workers,
                "workers_preloaded": preloaded_workers,
                "available_devices": available_devices,
                "workers": {worker_id: worker.device for worker_id, worker in self._workers.items()}
            })
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="WORKER_POOL_INIT_ERROR",
                details=f"Error initializing worker pool: {str(e)}",
                location="_initialize_worker_pool()"
            ))
            return Result.error(
                code="WORKER_POOL_INIT_ERROR",
                message="Failed to initialize worker pool",
                details={"error": str(e)}
            )
    
    async def _get_optimal_worker(self, model_id: str) -> Optional[str]:
        """Get the optimal worker for a given model.
        
        Args:
            model_id: ID of the model to be processed
            
        Returns:
            Worker ID or None if no suitable worker available
        """
        if not self._worker_pool_enabled or not self._workers:
            return None
        
        # For parallel processing, prefer distributing across different workers
        # Only prefer loaded models for single tasks, not parallel batches
        
        # Find idle worker with device affinity
        device_affinity = self.config.get("worker_pool.device_affinity", {})
        preferred_devices = device_affinity.get(model_id, [])
        
        # First, try workers on preferred devices
        if preferred_devices:
            for worker_id, worker in self._workers.items():
                if (worker.device in preferred_devices and 
                    worker.state == WorkerState.IDLE and
                    worker.is_running):
                    return worker_id
        
        # Fall back to any idle worker
        load_balancing = self.config.get("worker_pool.load_balancing", "round_robin")
        
        if load_balancing == "least_busy":
            # Find worker with least tasks processed
            idle_workers = [
                (worker_id, worker) for worker_id, worker in self._workers.items()
                if worker.state == WorkerState.IDLE and worker.is_running
            ]
            if idle_workers:
                return min(idle_workers, key=lambda x: x[1].tasks_processed)[0]
        else:
            # Round robin - distribute across all available workers
            idle_worker_ids = [
                worker_id for worker_id, worker in self._workers.items()
                if worker.state == WorkerState.IDLE and worker.is_running
            ]
            if idle_worker_ids:
                # Ensure we cycle through all workers
                if not hasattr(self, '_next_worker_index'):
                    self._next_worker_index = 0
                worker_id = idle_worker_ids[self._next_worker_index % len(idle_worker_ids)]
                self._next_worker_index += 1
                return worker_id
        
        return None
    
    async def _submit_task_to_worker_pool(self, task: WorkerTask) -> Optional[WorkerResult]:
        """Submit a task to the worker pool and wait for result.
        
        Args:
            task: Task to process
            
        Returns:
            Worker result or None if failed
        """
        if not self._worker_pool_enabled:
            return None
        
        # Use global job queue if available, otherwise fall back to old method
        if hasattr(self, '_global_job_queue') and self._global_job_queue:
            # Add task to global job queue
            await self._global_job_queue.put(task)
            logger.debug(f"Task {task.task_id} added to global job queue")
            
            # Wait for result with timeout using traditional result queue
            timeout = self.config.get("worker_pool.worker_timeout", 120)
            try:
                # Simple approach: wait for any result and check if it's ours
                start_time = time.time()
                while time.time() - start_time < timeout:
                    try:
                        result = await asyncio.wait_for(self._worker_result_queue.get(), timeout=5)
                        if result.task_id == task.task_id:
                            return result
                        else:
                            # Put back result for another caller
                            await self._worker_result_queue.put(result)
                            await asyncio.sleep(0.1)  # Small delay to prevent busy waiting
                    except asyncio.TimeoutError:
                        continue  # Try again
                
                logger.error(f"Timeout waiting for result for task {task.task_id} after {timeout}s")
                return None
            except Exception as e:
                logger.error(f"Error waiting for task result: {e}")
                return None
        else:
            # Fall back to old worker selection method
            worker_id = await self._get_optimal_worker(task.model_id)
            if not worker_id:
                logger.warning(f"No suitable worker available for task {task.task_id}")
                return None
            
            worker = self._workers[worker_id]
            if not await worker.submit_task(task):
                logger.error(f"Failed to submit task {task.task_id} to worker {worker_id}")
                return None
            
            # Wait for result
            timeout = self.config.get("worker_pool.worker_timeout", 120)
            try:
                result = await asyncio.wait_for(self._worker_result_queue.get(), timeout=timeout)
                return result
            except asyncio.TimeoutError:
                logger.error(f"Timeout waiting for result from worker {worker_id} for task {task.task_id}")
                return None
    
    async def _process_task_direct(self, task: WorkerTask, worker: 'ModelWorker') -> Optional[WorkerResult]:
        """Process task directly on worker - bypass all queue mechanisms."""
        try:
            logger.debug(f"Processing task {task.task_id} directly on worker {worker.worker_id}")
            
            # Call the worker's task processing method directly
            result = await worker._process_task(task)
            
            if result and result.success:
                logger.debug(f"Direct processing successful on worker {worker.worker_id}: {len(result.data.get('embeddings', []))} embeddings")
                return result
            else:
                logger.error(f"Direct processing failed on worker {worker.worker_id}: {result.error if result else 'No result'}")
                return None
                
        except Exception as e:
            logger.error(f"Exception in direct processing on worker {worker.worker_id}: {e}")
            return None
    
    async def get_worker_pool_status(self) -> Result:
        """Get detailed worker pool status.
        
        Returns:
            Worker pool status information
        """
        try:
            if not self._worker_pool_enabled:
                return Result.success(data={"worker_pool_enabled": False})
            
            worker_status = {}
            total_tasks = 0
            total_errors = 0
            total_switches = 0
            
            for worker_id, worker in self._workers.items():
                status = worker.get_status()
                worker_status[worker_id] = status
                total_tasks += status["tasks_processed"]
                total_errors += status["errors"]
                total_switches += status["model_switches"]
            
            return Result.success(data={
                "worker_pool_enabled": True,
                "total_workers": len(self._workers),
                "active_workers": sum(1 for w in self._workers.values() if w.is_running),
                "total_tasks_processed": total_tasks,
                "total_errors": total_errors,
                "total_model_switches": total_switches,
                "result_queue_size": self._worker_result_queue.qsize() if self._worker_result_queue else 0,
                "workers": worker_status
            })
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="WORKER_POOL_STATUS_ERROR",
                details=f"Error getting worker pool status: {str(e)}",
                location="get_worker_pool_status()"
            ))
            return Result.error(
                code="WORKER_POOL_STATUS_ERROR",
                message="Failed to get worker pool status",
                details={"error": str(e)}
            )
    
    async def generate_embeddings(self, texts: Union[str, List[str]], model_id: str = "embedding") -> Result:
        """Generate embeddings for text(s) using specified model."""
        try:
            # MUST use worker pool - no fallbacks
            if not self._worker_pool_enabled:
                return Result.error(
                    code="WORKER_POOL_DISABLED",
                    message="Worker pool is disabled - cannot generate embeddings"
                )
            
            if not self._workers:
                return Result.error(
                    code="NO_WORKERS_AVAILABLE", 
                    message="No workers available - cannot generate embeddings"
                )
            
            return await self._generate_embeddings_worker_pool(texts, model_id)
                
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="GENERATE_EMBEDDINGS_ERROR",
                details=f"Error generating embeddings: {str(e)}",
                location="generate_embeddings()"
            ))
            
            return Result.error(
                code="GENERATE_EMBEDDINGS_ERROR",
                message="Failed to generate embeddings",
                details={"model_id": model_id, "error": str(e)}
            )
    
    async def _generate_embeddings_worker_pool(self, texts: Union[str, List[str]], model_id: str) -> Result:
        """Generate embeddings using proper worker distribution like old working system."""
        try:
            # Normalize input to list
            text_list = [texts] if isinstance(texts, str) else texts
            
            # Check cache if enabled
            if self.config.get("embedding_cache.enabled", True):
                cache_result = await self._check_embedding_cache(text_list, model_id)
                if cache_result.success:
                    return cache_result
            
            # Get available workers for distribution (more lenient check)
            available_workers = [
                (worker_id, worker) for worker_id, worker in self._workers.items()
                if worker.is_running and worker.state != WorkerState.ERROR
            ]
            
            if not available_workers:
                return Result.error(
                    code="NO_AVAILABLE_WORKERS",
                    message=f"No workers available for distribution. Workers: {[(w_id, w.state.value, w.is_running) for w_id, w in self._workers.items()]}",
                    details={"worker_count": len(self._workers), "worker_states": {w_id: w.state.value for w_id, w in self._workers.items()}}
                )
            
            num_workers = len(available_workers)
            
            # OLD WORKING PATTERN: Distribute batch across available workers
            logger.info(f"Distributing batch of {len(text_list)} texts across {num_workers} workers")
            
            # Calculate batch size per worker (like old system)
            batch_size_per_worker = max(1, len(text_list) // num_workers)
            
            # Create tasks for each worker with their portion
            tasks = []
            worker_assignments = []
            
            for i in range(num_workers):
                start_idx = i * batch_size_per_worker
                # Last worker gets remaining texts
                end_idx = len(text_list) if i == num_workers - 1 else start_idx + batch_size_per_worker
                
                if start_idx < len(text_list):
                    batch_texts = text_list[start_idx:end_idx]
                    worker_id, worker = available_workers[i]
                    
                    task = WorkerTask(
                        task_id=f"distributed_batch_{i}_{worker_id}",
                        task_type="embedding",
                        model_id=model_id,
                        input_data=batch_texts,
                        metadata={
                            "batch_index": i,
                            "worker_assignment": worker_id,
                            "texts_count": len(batch_texts)
                        },
                        created_at=time.time()
                    )
                    
                    tasks.append(task)
                    worker_assignments.append((worker_id, worker, len(batch_texts)))
            
            # Submit tasks directly to assigned workers and collect results immediately
            logger.info(f"Submitting {len(tasks)} distributed tasks to workers")
            
            # Create direct worker processing tasks (bypass result queue entirely)
            worker_tasks = []
            for task, (worker_id, worker, batch_count) in zip(tasks, worker_assignments):
                logger.debug(f"Assigning {batch_count} texts to worker {worker_id}")
                # Process task directly on worker - no queues involved
                worker_task = self._process_task_direct(task, worker)
                worker_tasks.append(worker_task)
            
            # Wait for all workers to complete their batches in parallel
            results = await asyncio.gather(*worker_tasks, return_exceptions=True)
            
            # Collect results - filter out None and exceptions
            valid_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"Worker {worker_assignments[i][0]} failed with exception: {result}")
                    continue
                elif result is None:
                    logger.error(f"Worker {worker_assignments[i][0]} returned None - timeout or failure")
                    continue
                elif not result.success:
                    logger.error(f"Worker {worker_assignments[i][0]} returned failure: {result.error}")
                    continue
                else:
                    valid_results.append(result)
            
            # Merge all valid results
            all_embeddings = []
            worker_ids = []
            total_processing_time = 0
            
            for result in valid_results:
                all_embeddings.extend(result.data.get("embeddings", []))
                worker_ids.append(result.worker_id)
                total_processing_time = max(total_processing_time, result.processing_time)
            
            # Validate results
            expected_embeddings = len(text_list)
            actual_embeddings = len(all_embeddings)
            
            if not all_embeddings:
                return Result.error(
                    code="ALL_WORKERS_FAILED",
                    message=f"All {len(worker_assignments)} workers failed to process their batches",
                    details={"failed_workers": [w_id for w_id, _, _ in worker_assignments], "texts_count": len(text_list)}
                )
            
            if actual_embeddings != expected_embeddings:
                logger.warning(f"Embedding count mismatch: got {actual_embeddings}, expected {expected_embeddings}")
                # Still return partial results - better than failing completely
                logger.info(f"Returning partial results: {actual_embeddings}/{expected_embeddings} embeddings")
            
            # Cache results if enabled
            if self.config.get("embedding_cache.enabled", True):
                await self._cache_embeddings(text_list, all_embeddings, model_id)
            
            logger.info(f"Distributed processing complete: {len(all_embeddings)} embeddings from workers {worker_ids}")
            
            return Result.success(data={
                "embeddings": all_embeddings,
                "cached": False,
                "worker_ids": worker_ids,
                "parallel_workers": len(worker_ids),
                "processing_time": total_processing_time
            })
            
        except Exception as e:
            logger.error(f"Worker pool embedding error: {e}")
            return Result.error(
                code="WORKER_POOL_EXCEPTION",
                message=f"Worker pool failed with exception: {str(e)}",
                details={"error_type": type(e).__name__, "texts_count": len(text_list)}
            )
    
    async def _generate_embeddings_direct(self, texts: Union[str, List[str]], model_id: str) -> Result:
        """Generate embeddings directly (original method)."""
        try:
            # Prevent CPU usage - check device preference
            if (self.config.get("device_preference") == "cpu" and 
                self.config.get("worker_pool.require_gpu", True)):
                return Result.error(
                    code="CPU_USAGE_DISABLED",
                    message="REFUSING to run embeddings on CPU - GPU required. Fix GPU issues first.",
                    details={"device_preference": "cpu", "require_gpu": True}
                )
            # Get the model
            model_result = await self.get_embedding_model(model_id)
            if not model_result.success:
                return model_result
            
            model = model_result.data["model"]
            
            # Check cache if enabled
            if self.config.get("embedding_cache.enabled", True):
                cache_result = await self._check_embedding_cache(texts, model_id)
                if cache_result.success:
                    return cache_result
            
            # Generate embeddings
            if isinstance(texts, str):
                embeddings = model.encode([texts])
                result_embeddings = embeddings[0].tolist()
            else:
                embeddings = model.encode(texts)
                result_embeddings = [emb.tolist() for emb in embeddings]
            
            # Cache results if enabled
            if self.config.get("embedding_cache.enabled", True):
                await self._cache_embeddings(texts, result_embeddings, model_id)
            
            return Result.success(data={
                "embeddings": result_embeddings,
                "model_id": model_id,
                "dimension": len(result_embeddings[0] if isinstance(result_embeddings[0], list) else result_embeddings),
                "cached": False,
                "method": "direct"
            })
            
        except Exception as e:
            return Result.error(
                code="DIRECT_EMBEDDING_ERROR",
                message="Failed to generate embeddings directly",
                details={"model_id": model_id, "error": str(e)}
            )
    
    async def _check_embedding_cache(self, texts: Union[str, List[str]], model_id: str) -> Result:
        """Check if embeddings are cached."""
        try:
            if isinstance(texts, str):
                texts = [texts]
            
            cached_embeddings = []
            cache_hits = 0
            
            for text in texts:
                cache_key = self._get_cache_key(text, model_id)
                if cache_key in self._embedding_cache:
                    # Check TTL
                    if self._is_cache_valid(cache_key):
                        cached_embeddings.append(self._embedding_cache[cache_key])
                        cache_hits += 1
                    else:
                        # Cache expired
                        del self._embedding_cache[cache_key]
                        del self._cache_timestamps[cache_key]
                        return Result.error(code="CACHE_MISS", message="Cache expired")
                else:
                    return Result.error(code="CACHE_MISS", message="Not in cache")
            
            # All texts found in cache
            if len(cached_embeddings) == len(texts):
                return Result.success(data={
                    "embeddings": cached_embeddings[0] if len(cached_embeddings) == 1 else cached_embeddings,
                    "model_id": model_id,
                    "cached": True,
                    "cache_hits": cache_hits
                })
            
            return Result.error(code="PARTIAL_CACHE_HIT", message="Partial cache hit")
            
        except Exception as e:
            return Result.error(code="CACHE_CHECK_ERROR", message=str(e))
    
    async def _cache_embeddings(self, texts: Union[str, List[str]], embeddings: Union[List[float], List[List[float]]], model_id: str):
        """Cache embeddings for future use."""
        try:
            if isinstance(texts, str):
                texts = [texts]
                embeddings = [embeddings]
            
            current_time = time.time()
            max_cache_size = self.config.get("embedding_cache.max_cache_size", 10000)
            
            # Clean old cache entries if needed
            if len(self._embedding_cache) >= max_cache_size:
                await self._cleanup_embedding_cache()
            
            for text, embedding in zip(texts, embeddings):
                cache_key = self._get_cache_key(text, model_id)
                self._embedding_cache[cache_key] = embedding
                self._cache_timestamps[cache_key] = current_time
                
        except Exception as e:
            logger.warning(f"Failed to cache embeddings: {e}")
    
    def _get_cache_key(self, text: str, model_id: str) -> str:
        """Generate cache key for text and model."""
        content = f"{model_id}:{text}"
        return hashlib.md5(content.encode()).hexdigest()
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """Check if cache entry is still valid."""
        if cache_key not in self._cache_timestamps:
            return False
        
        ttl = self.config.get("embedding_cache.ttl_seconds", 3600)
        return (time.time() - self._cache_timestamps[cache_key]) < ttl
    
    async def _cleanup_embedding_cache(self):
        """Clean up old cache entries."""
        try:
            current_time = time.time()
            ttl = self.config.get("embedding_cache.ttl_seconds", 3600)
            
            expired_keys = [
                key for key, timestamp in self._cache_timestamps.items()
                if (current_time - timestamp) > ttl
            ]
            
            for key in expired_keys:
                del self._embedding_cache[key]
                del self._cache_timestamps[key]
            
            logger.debug(f"Cleaned up {len(expired_keys)} expired cache entries")
            
        except Exception as e:
            logger.warning(f"Cache cleanup failed: {e}")
    
    async def release_model(self, model_id: str) -> Result:
        """Release a reference to a model."""
        try:
            if model_id in self._loaded_models:
                model_ref = self._loaded_models[model_id]
                model_ref.remove_reference()
                
                logger.debug(f"Released reference to model {model_id}, remaining references: {model_ref.reference_count}")
                
                return Result.success(data={
                    "model_id": model_id,
                    "remaining_references": model_ref.reference_count
                })
            else:
                return Result.error(
                    code="MODEL_NOT_LOADED",
                    message=f"Model {model_id} is not currently loaded"
                )
                
        except Exception as e:
            return Result.error(
                code="RELEASE_MODEL_ERROR",
                message="Failed to release model",
                details={"model_id": model_id, "error": str(e)}
            )
    
    async def get_model_status(self) -> Result:
        """Get status of all loaded models."""
        try:
            model_status = {}
            for model_id, model_ref in self._loaded_models.items():
                model_status[model_id] = {
                    "references": model_ref.reference_count,
                    "last_accessed": model_ref.last_accessed,
                    "created_at": model_ref.created_at,
                    "idle_time": time.time() - model_ref.last_accessed,
                    "model_type": model_ref.model_config.get("type", "unknown")
                }
            
            return Result.success(data={
                "loaded_models": len(self._loaded_models),
                "cache_size": len(self._embedding_cache),
                "models": model_status
            })
            
        except Exception as e:
            return Result.error(
                code="STATUS_ERROR",
                message="Failed to get model status",
                details={"error": str(e)}
            )
    
    async def get_status(self) -> Result:
        """Get service status."""
        try:
            if self.initialized:
                model_status = await self.get_model_status()
                status_data = {
                    "module_id": MODULE_ID,
                    "initialized": self.initialized,
                    "config_loaded": bool(self.config)
                }
                if model_status.success:
                    status_data.update(model_status.data)
                return Result.success(data=status_data)
            else:
                return Result.success(data={
                    "module_id": MODULE_ID,
                    "initialized": False,
                    "config_loaded": bool(self.config)
                })
        except Exception as e:
            return Result.error(
                code="STATUS_ERROR",
                message="Failed to get service status",
                details={"error": str(e)}
            )
    
    async def scale_workers(self, target_count: int) -> Result:
        """Scale worker pool to target number of workers.
        
        Args:
            target_count: Target number of workers (0 to disable pool)
            
        Returns:
            Result with scaling status
        """
        try:
            if not self._worker_pool_enabled and target_count > 0:
                return Result.error(
                    code="WORKER_POOL_DISABLED",
                    message="Worker pool is not enabled"
                )
            
            current_count = len(self._workers)
            logger.info(f"Scaling workers from {current_count} to {target_count}")
            
            if target_count == 0:
                # Shutdown all workers
                logger.info("Shutting down all workers...")
                for worker_id, worker in self._workers.items():
                    try:
                        await worker.stop()
                        logger.info(f"Shut down worker {worker_id}")
                    except Exception as e:
                        logger.warning(f"Error shutting down worker {worker_id}: {e}")
                
                self._workers.clear()
                logger.info("All workers shut down - using direct model manager")
                
            elif target_count > current_count:
                # Scale up - add workers
                devices = self.config.get("worker_pool.devices", ["cuda:0", "cuda:1"])
                workers_to_add = target_count - current_count
                
                logger.info(f"Adding {workers_to_add} workers...")
                for i in range(workers_to_add):
                    device_idx = (current_count + i) % len(devices)
                    device = devices[device_idx]
                    worker_id = f"worker_{current_count + i}"
                    
                    try:
                        worker = ModelWorker(worker_id, device, self)
                        await worker.start()
                        self._workers[worker_id] = worker
                        
                        # Preload embedding if configured
                        if self.config.get("worker_pool.preload_embeddings", False):
                            await worker.switch_model("embedding")
                            worker.is_preloaded = True
                            logger.info(f"Added and preloaded worker {worker_id} on {device}")
                        else:
                            logger.info(f"Added worker {worker_id} on {device}")
                            
                    except Exception as e:
                        logger.error(f"Failed to add worker {worker_id}: {e}")
                        
            elif target_count < current_count:
                # Scale down - remove workers
                workers_to_remove = current_count - target_count
                worker_ids = list(self._workers.keys())
                
                logger.info(f"Removing {workers_to_remove} workers...")
                for i in range(workers_to_remove):
                    worker_id = worker_ids[-(i+1)]  # Remove from end
                    worker = self._workers[worker_id]
                    
                    try:
                        await worker.stop()
                        del self._workers[worker_id]
                        logger.info(f"Removed worker {worker_id}")
                    except Exception as e:
                        logger.error(f"Failed to remove worker {worker_id}: {e}")
            
            final_count = len(self._workers)
            logger.info(f"Worker scaling complete: {final_count} workers active")
            
            return Result.success(data={
                "previous_count": current_count,
                "target_count": target_count,
                "final_count": final_count,
                "workers": {worker_id: worker.device for worker_id, worker in self._workers.items()}
            })
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="WORKER_SCALING_ERROR",
                details=f"Error scaling workers: {str(e)}",
                location="scale_workers()"
            ))
            
            return Result.error(
                code="WORKER_SCALING_ERROR",
                message="Failed to scale workers",
                details={"target_count": target_count, "error": str(e)}
            )
    
    async def get_worker_status(self) -> Result:
        """Get detailed status of all workers.
        
        Returns:
            Result with worker status information
        """
        try:
            workers_status = {}
            for worker_id, worker in self._workers.items():
                workers_status[worker_id] = {
                    "device": worker.device,
                    "state": worker.state.value,
                    "current_model": worker.current_model_id,
                    "is_preloaded": worker.is_preloaded,
                    "tasks_processed": worker.tasks_processed,
                    "is_running": worker.is_running,
                    "last_activity": worker.last_activity
                }
            
            pool_status = {
                "worker_pool_enabled": self._worker_pool_enabled,
                "total_workers": len(self._workers),
                "active_workers": sum(1 for w in self._workers.values() if w.state == WorkerState.IDLE),
                "busy_workers": sum(1 for w in self._workers.values() if w.state == WorkerState.BUSY),
                "workers": workers_status
            }
            
            return Result.success(data=pool_status)
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="WORKER_STATUS_ERROR",
                details=f"Error getting worker status: {str(e)}",
                location="get_worker_status()"
            ))
            
            return Result.error(
                code="WORKER_STATUS_ERROR",
                message="Failed to get worker status",
                details={"error": str(e)}
            )

    async def shutdown(self) -> Result:
        """Shutdown the service gracefully and release GPU memory."""
        try:
            # Shutdown worker pool or main model manager (not both)
            if self._worker_pool_enabled:
                logger.info("Shutting down worker pool...")
                for worker_id, worker in self._workers.items():
                    try:
                        await worker.stop()
                        logger.info(f"Worker {worker_id} shut down")
                    except Exception as e:
                        logger.warning(f"Error shutting down worker {worker_id}: {e}")
                
                self._workers.clear()
                self._worker_pool_enabled = False
                self._worker_result_queue = None
                self._global_job_queue = None
                logger.info("Worker pool shutdown complete")
            else:
                # Traditional mode - clear main model manager models
                logger.info("Shutting down main model manager...")
                for model_id, model_ref in self._loaded_models.items():
                    try:
                        # Clear GPU memory for each model
                        if hasattr(model_ref.model_instance, 'to'):
                            model_ref.model_instance.to('cpu')
                        
                        logger.info(f"Released model: {model_id}")
                    except Exception as e:
                        logger.warning(f"Error releasing model {model_id}: {e}")
            
            # Clear CUDA cache if available
            try:
                import torch
                import gc
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("GPU memory cleared")
                gc.collect()
            except ImportError:
                pass
            except Exception as e:
                logger.warning(f"Error clearing GPU memory: {e}")
            
            # Clear all references
            self._loaded_models.clear()
            self._embedding_cache.clear()
            self._cache_timestamps.clear()
            
            self.initialized = False
            self.config = {}
            
            logger.info(f"{MODULE_ID} service shutdown complete")
            
            return Result.success(data={"status": "shutdown_complete"})
            
        except Exception as e:
            logger.error(error_message(
                module_id=MODULE_ID,
                error_type="SHUTDOWN_ERROR",
                details=f"Error during {MODULE_ID} service shutdown: {str(e)}",
                location="shutdown()"
            ))
            
            return Result.error(
                code="SHUTDOWN_ERROR",
                message=f"Failed to shutdown {MODULE_ID} service",
                details={"error": str(e)}
            )
    
    async def cleanup_resources(self):
        """
        Graceful resource cleanup - logging handled by decorator.
        Called during normal application shutdown via @graceful_shutdown decorator.
        """
        # Delegate to existing shutdown logic (ignore result for decorator pattern)
        await self.shutdown()
    
    def force_cleanup(self):
        """
        Force cleanup of resources - logging handled by decorator.
        Called during emergency shutdown via @force_shutdown decorator.
        """
        # Force cleanup of GPU resources and workers
        if self._worker_pool_enabled:
            # Force stop all workers synchronously
            import asyncio
            try:
                # Create new event loop if needed for cleanup
                loop = asyncio.get_event_loop()
                if loop.is_closed():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                
                # Force stop workers
                for worker in self._workers.values():
                    try:
                        worker.force_shutdown()  # Synchronous force shutdown
                    except Exception:
                        pass  # Ignore errors during force cleanup
            except Exception:
                pass
        
        # Clear all state
        self._workers.clear()
        self.initialized = False